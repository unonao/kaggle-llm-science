{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18128c95-abdd-4dc2-988d-43b245653115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/working'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /tmp/working\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed79bfe7-9bc7-446d-b222-83500e93f1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ctypes\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from collections.abc import Iterable\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import blingfire as bf\n",
    "import faiss\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faiss import read_index, write_index\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422bf6e8-6b0c-485b-90d8-8d5f6d7267d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce61f7df-1031-49ee-bc31-ec7bc5f0734d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 8)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"input/kaggle-llm-science-exam/train.csv\")\n",
    "print(f\"{df.shape}\")\n",
    "\n",
    "\n",
    "# モデル読み込み\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "model.max_seq_length = 384\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e737c5b-7d88-49ca-ab53-3d176a57b710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【title 検索】\n",
      "read_index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31aa6e482d3c4781bc765432fef5765f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relevant_title_retrieval(\n",
    "    df: pd.DataFrame,\n",
    "    index_path: str,\n",
    "    model: SentenceTransformer,\n",
    "    top_k: int = 3,\n",
    "    batch_size: int = 32,\n",
    ") -> pd.DataFrame:\n",
    "    print(\"read_index\")\n",
    "    sentence_index = read_index(index_path)  # index 読み込み\n",
    "    prompt_embeddings = model.encode(\n",
    "        df.prompt.values,\n",
    "        batch_size=batch_size,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "    search_score, search_index = sentence_index.search(prompt_embeddings, top_k)\n",
    "    del sentence_index\n",
    "    del prompt_embeddings\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return search_score, search_index\n",
    "\n",
    "\n",
    "# title 検索\n",
    "print(\"【title 検索】\")\n",
    "search_score, search_index = relevant_title_retrieval(\n",
    "    df,\n",
    "    \"input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\",\n",
    "    model,\n",
    "    top_k=3,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e7aead5-8f91-4637-aaff-b07234541e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【wikipedia file data 取得】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4b791e42134052b217c413244b86bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  prompt_id       file\n",
      "0      1141        151  a.parquet\n",
      "1  11963992        185  a.parquet\n",
      "2      1200         63  a.parquet\n",
      "3      1234        130  a.parquet\n",
      "4      1317         89  a.parquet\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_file_data(\n",
    "    search_score: np.ndarray,\n",
    "    search_index: np.ndarray,\n",
    "    wiki_index_path: str,\n",
    ") -> pd.DataFrame:\n",
    "    wiki_index_df = pd.read_parquet(wiki_index_path, columns=[\"id\", \"file\"])\n",
    "    wikipedia_file_data = []\n",
    "    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n",
    "        _df = wiki_index_df.loc[idx].copy()\n",
    "        _df[\"prompt_id\"] = i\n",
    "        wikipedia_file_data.append(_df)\n",
    "    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "    wikipedia_file_data = (\n",
    "        wikipedia_file_data[[\"id\", \"prompt_id\", \"file\"]]\n",
    "        .drop_duplicates()\n",
    "        .sort_values([\"file\", \"id\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    ## Save memory - delete df since it is no longer necessary\n",
    "    del wiki_index_df\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return wikipedia_file_data\n",
    "\n",
    "\n",
    "print(\"【wikipedia file data 取得】\")\n",
    "wikipedia_file_data = get_wikipedia_file_data(\n",
    "    search_score,\n",
    "    search_index,\n",
    "    \"input/wikipedia-20230701/wiki_2023_index.parquet\",\n",
    ")\n",
    "print(wikipedia_file_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b0b37-ccd0-4644-a0aa-e6203f3c77d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【wikipedia text data 取得】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bd1e0e6b09423bb79a00b6ca9c4911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_full_text_data(\n",
    "    wikipedia_file_data: pd.DataFrame,\n",
    "    wiki_dir: str,\n",
    "):\n",
    "    ## Get the full text data\n",
    "    wiki_text_data = []\n",
    "    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n",
    "        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data[\"file\"] == file][\"id\"].tolist()]\n",
    "        _df = pd.read_parquet(f\"{wiki_dir}/{file}\", columns=[\"id\", \"text\"])\n",
    "        _df_temp = _df[_df[\"id\"].isin(_id)].copy()\n",
    "        del _df\n",
    "        _ = gc.collect()\n",
    "        libc.malloc_trim(0)\n",
    "        wiki_text_data.append(_df_temp)\n",
    "    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return wiki_text_data\n",
    "\n",
    "\n",
    "# wikipedia text data 取得 (\"id\", \"text\")\n",
    "print(\"【wikipedia text data 取得】\")\n",
    "wiki_text_data = get_full_text_data(\n",
    "    wikipedia_file_data,\n",
    "    \"input/wikipedia-20230701\",\n",
    ")\n",
    "print(wiki_text_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62ebbca4-e8e8-485e-859b-9aaab916d84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ca672a13f7477b83af09f9353be12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82eb7e66ab14edaa024845e2d47b37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103529</td>\n",
       "      <td>* Tri- is a numerical prefix meaning three.</td>\n",
       "      <td>(0, 43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103529</td>\n",
       "      <td>Tri or TRI may also refer to: ==Places== * Tri...</td>\n",
       "      <td>(44, 946)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12571</td>\n",
       "      <td>Galaxy groups and clusters are the largest kno...</td>\n",
       "      <td>(0, 148)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12571</td>\n",
       "      <td>They form the densest part of the large-scale ...</td>\n",
       "      <td>(149, 221)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12571</td>\n",
       "      <td>In models for the gravitational formation of s...</td>\n",
       "      <td>(222, 405)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>8603</td>\n",
       "      <td>When looking at a cross section of a beam of l...</td>\n",
       "      <td>(32636, 32776)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>8603</td>\n",
       "      <td>In the case of Young's double-slit experiment,...</td>\n",
       "      <td>(32777, 33018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>8603</td>\n",
       "      <td>In the case of particles like electrons, neutr...</td>\n",
       "      <td>(33019, 33388)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>8603</td>\n",
       "      <td>These femtosecond-duration pulses will allow f...</td>\n",
       "      <td>(33389, 33498)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>8603</td>\n",
       "      <td>Due to these short pulses, radiation damage ca...</td>\n",
       "      <td>(33499, 33647)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>702 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id                                               text  \\\n",
       "0        103529        * Tri- is a numerical prefix meaning three.   \n",
       "1        103529  Tri or TRI may also refer to: ==Places== * Tri...   \n",
       "2         12571  Galaxy groups and clusters are the largest kno...   \n",
       "3         12571  They form the densest part of the large-scale ...   \n",
       "4         12571  In models for the gravitational formation of s...   \n",
       "..          ...                                                ...   \n",
       "697        8603  When looking at a cross section of a beam of l...   \n",
       "698        8603  In the case of Young's double-slit experiment,...   \n",
       "699        8603  In the case of particles like electrons, neutr...   \n",
       "700        8603  These femtosecond-duration pulses will allow f...   \n",
       "701        8603  Due to these short pulses, radiation damage ca...   \n",
       "\n",
       "             offset  \n",
       "0           (0, 43)  \n",
       "1         (44, 946)  \n",
       "2          (0, 148)  \n",
       "3        (149, 221)  \n",
       "4        (222, 405)  \n",
       "..              ...  \n",
       "697  (32636, 32776)  \n",
       "698  (32777, 33018)  \n",
       "699  (33019, 33388)  \n",
       "700  (33389, 33498)  \n",
       "701  (33499, 33647)  \n",
       "\n",
       "[702 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentencize(\n",
    "    documents: Iterable[str],\n",
    "    document_ids: Iterable,\n",
    "    offsets: Iterable[tuple[int, int]],\n",
    "    filter_len: int = 3,\n",
    "    disable_progress_bar: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into sentences. Can be used with `sectionize_documents`\n",
    "    to further split documents into more manageable pieces. Takes in offsets\n",
    "    to ensure that after splitting, the sentences can be matched to the\n",
    "    location in the original documents.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param offsets: Iterable tuple of the start and end indices\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    document_sentences = []\n",
    "    for document, document_id, offset in tqdm(\n",
    "        zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar\n",
    "    ):\n",
    "        # 不要と思われる部分は削除する\n",
    "        document = document.split(\"==See also==\")[0]\n",
    "        document = document.split(\"== See also ==\")[0]\n",
    "        document = document.split(\"==References==\")[0]\n",
    "        document = document.split(\"== References ==\")[0]\n",
    "        document = document.split(\"==Further reading==\")[0]\n",
    "        document = document.split(\"== Further reading ==\")[0]\n",
    "        document = document.split(\"==External links==\")[0]\n",
    "        document = document.split(\"== External links ==\")[0]\n",
    "        try:\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            for o in sentence_offsets:\n",
    "                if o[1] - o[0] > filter_len:\n",
    "                    sentence = document[o[0] : o[1]]\n",
    "                    abs_offsets = (o[0] + offset[0], o[1] + offset[0])\n",
    "                    row = {}\n",
    "                    row[\"document_id\"] = document_id\n",
    "                    row[\"text\"] = sentence\n",
    "                    row[\"offset\"] = abs_offsets\n",
    "                    document_sentences.append(row)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(document_sentences)\n",
    "\n",
    "\n",
    "def sectionize_documents(\n",
    "    documents: Iterable[str], document_ids: Iterable, disable_progress_bar: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the\n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for document_id, document in tqdm(\n",
    "        zip(document_ids, documents), total=len(documents), disable=disable_progress_bar\n",
    "    ):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row[\"document_id\"] = document_id\n",
    "        row[\"text\"] = text\n",
    "        row[\"offset\"] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values([\"document_id\", \"offset\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def process_documents(\n",
    "    documents: Iterable[str],\n",
    "    document_ids: Iterable,\n",
    "    split_sentences: bool = True,\n",
    "    filter_len: int = 3,\n",
    "    disable_progress_bar: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main helper function to process documents from the EMR.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param document_type: String denoting the document type to be processed\n",
    "    :param document_sections: List of sections for a given document type to process\n",
    "    :param split_sentences: Flag to determine whether to further split sections into sentences\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n",
    "\n",
    "    if split_sentences:\n",
    "        df = sentencize(df.text.values, df.document_id.values, df.offset.values, filter_len, disable_progress_bar)\n",
    "    return df\n",
    "\n",
    "\n",
    "processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n",
    "processed_wiki_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bccab020-09f5-4665-b1a9-e4758502333c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            * Tri- is a numerical prefix meaning three.\n",
       "1      Tri or TRI may also refer to: ==Places== * Tri...\n",
       "2      Galaxy groups and clusters are the largest kno...\n",
       "3      They form the densest part of the large-scale ...\n",
       "4      In models for the gravitational formation of s...\n",
       "                             ...                        \n",
       "697    When looking at a cross section of a beam of l...\n",
       "698    In the case of Young's double-slit experiment,...\n",
       "699    In the case of particles like electrons, neutr...\n",
       "700    These femtosecond-duration pulses will allow f...\n",
       "701    Due to these short pulses, radiation damage ca...\n",
       "Name: text, Length: 702, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_wiki_text_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "118626a4-8af2-464b-bd6f-7515779ac76d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Get embeddings of the wiki text data】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b577901711aa4ed6a064d314bf68491d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Get embeddings of the wiki text data\n",
    "print(\"【Get embeddings of the wiki text data】\")\n",
    "wiki_data_embeddings = model.encode(\n",
    "    processed_wiki_text_data.text,\n",
    "    batch_size=16,\n",
    "    device=\"cuda\",\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True,\n",
    ")  # .half()\n",
    "wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "001f5f61-4ec5-444b-b8ce-3a75c3686b70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Combine all answers】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1d57d133ba43718316ee710064d2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de19a2c0bc4f4af694e558561a03a076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71a8eb981234efd86002f4c829222a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab8f8f0e52f42878b6baf00b4633f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38b5d90957547348c268a8d0c96e0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0959c36c2ad7426e8313cd4710dfe4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Combine all answers\n",
    "print(\"【Combine all answers】\")\n",
    "df[\"answer_all\"] = df.apply(lambda x: \" \".join([x[\"A\"], x[\"B\"], x[\"C\"], x[\"D\"], x[\"E\"]]), axis=1)\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt\"] + \" \" + df[\"answer_all\"]\n",
    "\n",
    "option_embeddings = []\n",
    "for letter in [\"prompt_answer_stem\", \"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "    embeddings = model.encode(\n",
    "        df[letter].values,\n",
    "        batch_size=16,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    embeddings = embeddings.detach().cpu().numpy()\n",
    "    option_embeddings.append(embeddings)\n",
    "df.drop([\"answer_all\", \"prompt_answer_stem\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "af5cbc99-b221-40fe-b504-7772d79b7a64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Extract contexts from matching pairs】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e820d7fc180642c9a6ab1d89292ec195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_contexts_from_matching_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    processed_wiki_text_data: pd.DataFrame,\n",
    "    wikipedia_file_data: pd.DataFrame,\n",
    "    wiki_data_embeddings: np.ndarray,\n",
    "    option_embeddings: list[np.ndarray],\n",
    "    num_sentences_include: int = 5,\n",
    "):\n",
    "    contexts = []\n",
    "    for r in tqdm(df.itertuples(), total=len(df)):\n",
    "        prompt_id = r.Index\n",
    "        prompt_indices = processed_wiki_text_data[\n",
    "            processed_wiki_text_data[\"document_id\"].isin(\n",
    "                wikipedia_file_data[wikipedia_file_data[\"prompt_id\"] == prompt_id][\"id\"].values\n",
    "            )\n",
    "        ].index.values\n",
    "        \"\"\"\n",
    "        if prompt_indices.shape[0] > 0:\n",
    "            prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "            prompt_index.add(wiki_data_embeddings[prompt_indices])\n",
    "            # option embeddings のそれぞれで検索した結果を結合し、全体で上位5つを取得する\n",
    "            search_results_dict = {}\n",
    "            for embeddings in option_embeddings:\n",
    "                ss, ii = prompt_index.search(embeddings, num_sentences_include)\n",
    "                for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n",
    "                    if _i in search_results_dict:\n",
    "                        search_results_dict[_i] = min(_s, search_results_dict[_i])\n",
    "                    else:\n",
    "                        search_results_dict[_i] = _s\n",
    "            search_results = sorted(search_results_dict.items(), key=lambda x: x[1])[:num_sentences_include]\n",
    "            context = \"\"\n",
    "            for _i, _s in search_results:\n",
    "                context += processed_wiki_text_data.loc[prompt_indices][\"text\"].iloc[_i] + \" \"\n",
    "        contexts.append(context)\n",
    "        \"\"\"\n",
    "        if prompt_indices.shape[0] > 0:\n",
    "            prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "            prompt_index.add(wiki_data_embeddings[prompt_indices])\n",
    "            # option embeddings のそれぞれで検索した結果を結合し、全体で上位5つを取得する\n",
    "            search_results_dict = {}\n",
    "            for embeddings in option_embeddings:\n",
    "                ss, ii = prompt_index.search(embeddings[np.newaxis, prompt_id], num_sentences_include)\n",
    "                for _s, _i in zip(ss[0], ii[0]):\n",
    "                    if _i in search_results_dict:\n",
    "                        search_results_dict[_i] = min(_s, search_results_dict[_i])\n",
    "                    else:\n",
    "                        search_results_dict[_i] = _s\n",
    "            search_results = sorted(search_results_dict.items(), key=lambda x: x[1])[:num_sentences_include]\n",
    "            context = \"\"\n",
    "            for _i, _s in search_results:\n",
    "                context += processed_wiki_text_data.loc[prompt_indices][\"text\"].iloc[_i] + \" \"\n",
    "        contexts.append(context)\n",
    "    return contexts\n",
    "\n",
    "\n",
    "## Extract contexts from matching pairs\n",
    "print(\"【Extract contexts from matching pairs】\")\n",
    "contexts = extract_contexts_from_matching_pairs(\n",
    "    df,\n",
    "    processed_wiki_text_data,\n",
    "    wikipedia_file_data,\n",
    "    wiki_data_embeddings,\n",
    "    option_embeddings,\n",
    "    num_sentences_include=10,\n",
    ")\n",
    "df[\"context\"] = contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a99883-26ff-4c76-be82-2dcb1ca7a668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "46434f6b-dc22-4343-aba0-021ffc8425c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "option_embeddings[0][:, 1, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "28ff2c64-4f97-49ae-8c5c-8f58bd0782ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_df(df, index):\n",
    "    for col, row in zip(df.columns, df.iloc[index]):\n",
    "        print(f\"【{col}】:\", row)\n",
    "\n",
    "\n",
    "old_df = pd.read_csv(\"preprocessed/000_base/000/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f07c9c91-6063-403f-bf34-928fbe72662e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【id】: 2\n",
      "【prompt】: Which of the following statements accurately describes the origin and significance of the triskeles symbol?\n",
      "【A】: The triskeles symbol was reconstructed as a feminine divine triad by the rulers of Syracuse, and later adopted as an emblem. Its usage may also be related to the Greek name of Sicily, Trinacria, which means \"having three headlands.\" The head of Medusa at the center of the Sicilian triskeles represents the three headlands.\n",
      "【B】: The triskeles symbol is a representation of three interlinked spirals, which was adopted as an emblem by the rulers of Syracuse. Its usage in modern flags of Sicily has its origins in the ancient Greek name for the island, Trinacria, which means \"Sicily with three corners.\" The head of Medusa at the center is a representation of the island's rich cultural heritage.\n",
      "【C】: The triskeles symbol is a representation of a triple goddess, reconstructed by the rulers of Syracuse, who adopted it as an emblem. Its significance lies in the fact that it represents the Greek name for Sicily, Trinacria, which contains the element \"tria,\" meaning three. The head of Medusa at the center of the Sicilian triskeles represents the three headlands.\n",
      "【D】: The triskeles symbol represents three interlocked spiral arms, which became an emblem for the rulers of Syracuse. Its usage in modern flags of Sicily is due to the island's rich cultural heritage, which dates back to ancient times. The head of Medusa at the center represents the lasting influence of Greek mythology on Sicilian culture.\n",
      "【E】: The triskeles symbol is a representation of the Greek goddess Hecate, reconstructed by the rulers of Syracuse. Its adoption as an emblem was due to its cultural significance, as it represented the ancient Greek name for Sicily, Trinacria. The head of Medusa at the center of the Sicilian triskeles represents the island's central location in the Mediterranean.\n",
      "【answer】: A\n",
      "【context】: It is possible that this usage is related with the Greek name of the island of Sicily, Trinacria (Τρινακρία \"having three headlands\").Liddell and Scott’s Greek-English Lexicon (A Lexicon Abridged from), Oxford, 1944, p.27, Cassell's Latin Dictionary, Marchant, J.R.V, & Charles, Joseph F., (Eds.), Revised Edition, 1928 The Sicilian triskeles is shown with the head of Medusa at the center.Matthews, Jeff (2005) Symbols of Naples The ancient symbol has been re-introduced in modern flags of Sicily since 1848. An early flag of Sicily, proposed in 1848, included the Sicilian triskeles or \"Trinacria symbol\". The triskeles was adopted as emblem by the rulers of Syracuse. The actual triskeles symbol of three human legs is found especially in Greek antiquity, beginning in archaic pottery and continued in coinage of the classical period. Also p. 134: [On CRs] \"Using Celtic symbols such as triskeles and spirals\" Other uses of triskelion-like emblems include the logo for the Trisquel Linux distribution and the seal of the United States Department of Transportation. The oldest find of a triskeles in Sicily is a vase dated to 700 BCE, for which researchers assume a Minoan-Mycenaean origin. ===Roman period and Late Antiquity=== Late examples of the triple spiral symbols are found in Iron Age Europe, e.g. carved in rock in Castro Culture settlement in Galicia, Asturias and Northern Portugal. The triskelion was a motif in the art of the Iron age Celtic La Tène culture. ===Classical Antiquity=== The triskeles proper, composed of three human legs, is younger than the triple spiral, found in decorations on Greek pottery especially as a design shown on hoplite shields, and later also minted on Greek and Anatolian coinage. The spiral triskele is one of the primary symbols of Celtic Reconstructionist Paganism, used to represent a variety of triplicities in cosmology and theology; it is also a favored symbol due to its association with the god Manannán mac Lir.Bonewits, Isaac (2006) Bonewits's Essential Guide to Druidism. thumb|Neolithic triple spiral symbol A triskelion or triskeles is an ancient motif consisting of a triple spiral exhibiting rotational symmetry or other patterns in triplicate that emanate from a common center. The three legs (triskeles) symbol is rarely found as a charge in late medieval heraldry, notably as the arms of the King of Mann (Armorial Wijnbergen, ), and as canting arms in the city seal of the Bavarian city of Füssen (dated 1317). ==Modern usage== The triskeles was included in the design of the Army Gold Medal awarded to British Army majors and above who had taken a key part in the Battle of Maida (1806).Charles Norton Elvin, A Dictionary of Heraldry (1889), p. 126. \n",
      "\n",
      "【id】: 2\n",
      "【prompt】: Which of the following statements accurately describes the origin and significance of the triskeles symbol?\n",
      "【A】: The triskeles symbol was reconstructed as a feminine divine triad by the rulers of Syracuse, and later adopted as an emblem. Its usage may also be related to the Greek name of Sicily, Trinacria, which means \"having three headlands.\" The head of Medusa at the center of the Sicilian triskeles represents the three headlands.\n",
      "【B】: The triskeles symbol is a representation of three interlinked spirals, which was adopted as an emblem by the rulers of Syracuse. Its usage in modern flags of Sicily has its origins in the ancient Greek name for the island, Trinacria, which means \"Sicily with three corners.\" The head of Medusa at the center is a representation of the island's rich cultural heritage.\n",
      "【C】: The triskeles symbol is a representation of a triple goddess, reconstructed by the rulers of Syracuse, who adopted it as an emblem. Its significance lies in the fact that it represents the Greek name for Sicily, Trinacria, which contains the element \"tria,\" meaning three. The head of Medusa at the center of the Sicilian triskeles represents the three headlands.\n",
      "【D】: The triskeles symbol represents three interlocked spiral arms, which became an emblem for the rulers of Syracuse. Its usage in modern flags of Sicily is due to the island's rich cultural heritage, which dates back to ancient times. The head of Medusa at the center represents the lasting influence of Greek mythology on Sicilian culture.\n",
      "【E】: The triskeles symbol is a representation of the Greek goddess Hecate, reconstructed by the rulers of Syracuse. Its adoption as an emblem was due to its cultural significance, as it represented the ancient Greek name for Sicily, Trinacria. The head of Medusa at the center of the Sicilian triskeles represents the island's central location in the Mediterranean.\n",
      "【answer】: A\n",
      "【context】: It is possible that this usage is related with the Greek name of the island of Sicily, Trinacria (Τρινακρία \"having three headlands\").Liddell and Scott’s Greek-English Lexicon (A Lexicon Abridged from), Oxford, 1944, p.27, Cassell's Latin Dictionary, Marchant, J.R.V, & Charles, Joseph F., (Eds.), Revised Edition, 1928 The Sicilian triskeles is shown with the head of Medusa at the center.Matthews, Jeff (2005) Symbols of Naples The ancient symbol has been re-introduced in modern flags of Sicily since 1848. An early flag of Sicily, proposed in 1848, included the Sicilian triskeles or \"Trinacria symbol\". The triskeles was adopted as emblem by the rulers of Syracuse. The oldest find of a triskeles in Sicily is a vase dated to 700 BCE, for which researchers assume a Minoan-Mycenaean origin. ===Roman period and Late Antiquity=== Late examples of the triple spiral symbols are found in Iron Age Europe, e.g. carved in rock in Castro Culture settlement in Galicia, Asturias and Northern Portugal. In the Hellenistic period, the symbol becomes associated with the island of Sicily, appearing on coins minted under Dionysius I of Syracuse beginning in BCE.Arthur Bernard Cook, Zeus: a study in ancient religion, Volume 3, Part 2 (1940), p. 1074. \n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "print_df(df, index)\n",
    "print()\n",
    "print_df(old_df, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df7a31-3e8c-4e39-bd67-514802ad36b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
