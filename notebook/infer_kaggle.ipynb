{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07be3ebd-dda5-4a32-bb66-e2ba8f0125bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/working'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7265eae-4a8c-415d-8a71-9c63ea0f9afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieval\n",
    "class Config:\n",
    "    sim_model = \"BAAI/bge-small-en\"\n",
    "    sim_max_length = 384\n",
    "\n",
    "    batch_size = 32\n",
    "    num_sentences_include = 20\n",
    "    doc_top_k = 3\n",
    "    window_size = 5\n",
    "    sliding_size = 4\n",
    "\n",
    "    # index\n",
    "    wiki_index_path = \"preprocessed/320_doc_index/001/all.parquet\"\n",
    "    index_path = \"preprocessed/320_doc_index/001/ivfpq_index.faiss\"\n",
    "\n",
    "    # wiki b\n",
    "    wiki_dir = \"input/llm-science-wikipedia-data-b\"\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e9e88d-d741-4d2b-80e1-f755603bad3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import blingfire as bf\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from faiss import read_index, write_index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e82669-cc91-4254-a4c9-8e8fa84701a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_chunk_by_sliding_window(text_list: list[str], window_size: int, sliding_size: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    text のリストをsliding windowで結合する。window_size個のtextが含まれるまで結合し、sliding_size個ずつずらして結合する。\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text_list), sliding_size):\n",
    "        chunk = \" \".join(text_list[i : i + window_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_sections(text: str) -> list[tuple[str, str]]:\n",
    "    pattern = re.compile(r\"#{2,}\\s?(.*?)\\s?#{2,}\")\n",
    "    sections = []\n",
    "\n",
    "    matches = list(pattern.finditer(text))\n",
    "    start_idx = 0\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        sections.append((\"\", text))\n",
    "        return sections\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        if i == 0:\n",
    "            end_idx = match.start()\n",
    "            sections.append((\"\", text[start_idx:end_idx].strip()))\n",
    "\n",
    "        start_idx = match.end()\n",
    "        end_idx = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "        section = (match.group(1).strip(), text[start_idx:end_idx].strip())\n",
    "        if section[0] not in [\"See also\", \"References\", \"Further reading\", \"External links\"]:\n",
    "            sections.append(section)\n",
    "\n",
    "        start_idx = end_idx\n",
    "\n",
    "    # 空のtextの場合は飛ばす\n",
    "    sections = [section for section in sections if len(section[1].split(\" \")) >= 3]\n",
    "    return sections\n",
    "\n",
    "\n",
    "def sentencize(\n",
    "    titles: Iterable[str],\n",
    "    documents: Iterable[str],\n",
    "    document_ids: Iterable,\n",
    "    window_size: int = 3,\n",
    "    sliding_size: int = 2,\n",
    "    filter_len: int = 5,\n",
    "    filter_len_max: int = 500,\n",
    "    disable_progress_bar: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into sentences. Can be used with `sectionize_documents`\n",
    "    to further split documents into more manageable pieces. Takes in offsets\n",
    "    to ensure that after splitting, the sentences can be matched to the\n",
    "    location in the original documents.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    document_sentences = []\n",
    "    for title, document, document_id in tqdm(\n",
    "        zip(titles, documents, document_ids), total=len(documents), disable=disable_progress_bar\n",
    "    ):\n",
    "        try:\n",
    "            # chunk にまとめる\n",
    "            ## 念のため改行をスペースに変換\n",
    "            document = document.replace(\"\\n\", \" \")\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            section_sentences = []\n",
    "            for o in sentence_offsets:\n",
    "                if filter_len < o[1] - o[0] and o[1] - o[0] < filter_len_max:\n",
    "                    section_sentences.append(document[o[0] : o[1]])\n",
    "            chunks = extract_chunk_by_sliding_window(section_sentences, window_size, sliding_size)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                row = {}\n",
    "                row[\"document_id\"] = document_id\n",
    "                row[\"text\"] = f\"{title} > {chunk}\"\n",
    "                row[\"offset\"] = (0, 0)\n",
    "                document_sentences.append(row)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(document_sentences)\n",
    "\n",
    "\n",
    "def sectionize_documents(\n",
    "    titles: Iterable[str],\n",
    "    documents: Iterable[str],\n",
    "    document_ids: Iterable,\n",
    "    disable_progress_bar: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the\n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for title, document_id, document in tqdm(\n",
    "        zip(titles, document_ids, documents), total=len(documents), disable=disable_progress_bar\n",
    "    ):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row[\"document_id\"] = document_id\n",
    "        row[\"text\"] = text\n",
    "        row[\"offset\"] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values([\"document_id\", \"offset\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def relevant_title_retrieval(\n",
    "    df: pd.DataFrame,\n",
    "    index_path: str,\n",
    "    model: SentenceTransformer,\n",
    "    top_k: int = 3,\n",
    "    batch_size: int = 32,\n",
    ") -> pd.DataFrame:\n",
    "    sentence_index = read_index(index_path)  # index 読み込み\n",
    "    # res = faiss.StandardGpuResources()  # use a single GPU\n",
    "    # co = faiss.GpuClonerOptions()\n",
    "    # co.useFloat16 = True\n",
    "    # sentence_index = faiss.index_cpu_to_gpu(res, 0, sentence_index, co)\n",
    "    sentence_index.nprobe = 10\n",
    "    prompt_embeddings = model.encode(\n",
    "        df.prompt_answer_stem.values,\n",
    "        batch_size=batch_size,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        # convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    prompt_embeddings = prompt_embeddings.astype(np.float32)\n",
    "    # prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "    search_score, search_index = sentence_index.search(prompt_embeddings, top_k)\n",
    "    # res.noTempMemory()\n",
    "    # del res\n",
    "    del sentence_index\n",
    "    del prompt_embeddings\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return search_score, search_index\n",
    "\n",
    "\n",
    "def get_wikipedia_file_data(\n",
    "    search_score: np.ndarray,\n",
    "    search_index: np.ndarray,\n",
    "    wiki_index_path: str,\n",
    ") -> pd.DataFrame:\n",
    "    wiki_index_df = pd.read_parquet(wiki_index_path, columns=[\"id\", \"file\"])\n",
    "    wikipedia_file_data = []\n",
    "    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n",
    "        _df = wiki_index_df.loc[idx].copy()\n",
    "        _df[\"prompt_id\"] = i\n",
    "        wikipedia_file_data.append(_df)\n",
    "    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "    wikipedia_file_data = (\n",
    "        wikipedia_file_data[[\"id\", \"prompt_id\", \"file\"]]\n",
    "        .drop_duplicates()\n",
    "        .sort_values([\"file\", \"id\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    ## Save memory - delete df since it is no longer necessary\n",
    "    del wiki_index_df\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return wikipedia_file_data\n",
    "\n",
    "\n",
    "def get_full_text_data(\n",
    "    wikipedia_file_data: pd.DataFrame,\n",
    "    wiki_dir: str,\n",
    "):\n",
    "    ## Get the full text data\n",
    "    wiki_text_data = []\n",
    "    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n",
    "        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data[\"file\"] == file][\"id\"].tolist()]\n",
    "        _df = pd.read_parquet(f\"{wiki_dir}/{file}\", columns=[\"id\", \"title\", \"text\"])\n",
    "        _df_temp = _df[_df[\"id\"].isin(_id)].copy()\n",
    "        del _df\n",
    "        _ = gc.collect()\n",
    "        libc.malloc_trim(0)\n",
    "        wiki_text_data.append(_df_temp)\n",
    "    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return wiki_text_data\n",
    "\n",
    "\n",
    "def extract_contexts_from_matching_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    processed_wiki_text_data: pd.DataFrame,\n",
    "    wikipedia_file_data: pd.DataFrame,\n",
    "    wiki_data_embeddings: np.ndarray,\n",
    "    question_embeddings: np.ndarray,\n",
    "    num_sentences_include: int = 5,\n",
    "):\n",
    "    results = {\"contexts\": [], \"sim_min\": [], \"sim_max\": [], \"sim_mean\": [], \"sim_std\": [], \"sim_num\": []}\n",
    "    for r in tqdm(df.itertuples(), total=len(df)):\n",
    "        prompt_id = r.Index\n",
    "        prompt_indices = processed_wiki_text_data[\n",
    "            processed_wiki_text_data[\"document_id\"].isin(\n",
    "                wikipedia_file_data[wikipedia_file_data[\"prompt_id\"] == prompt_id][\"id\"].values\n",
    "            )\n",
    "        ].index.values\n",
    "        assert prompt_indices.shape[0] > 0\n",
    "        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "        prompt_index.add(wiki_data_embeddings[prompt_indices])\n",
    "        ## Get the top matches\n",
    "        ss, ii = prompt_index.search(question_embeddings[np.newaxis, prompt_id], num_sentences_include)\n",
    "        context = \"\"\n",
    "        total_len = 0\n",
    "        num = 0\n",
    "        for _s, _i in zip(ss[0], ii[0]):\n",
    "            if total_len > 1000 or _s >= 1.0:\n",
    "                break\n",
    "            text = processed_wiki_text_data.loc[prompt_indices][\"text\"].iloc[_i]\n",
    "            context += text + \" \"\n",
    "            total_len += len(text.split(\" \"))\n",
    "            num += 1\n",
    "        results[\"contexts\"].append(context)\n",
    "        results[\"sim_max\"].append(ss[0][:num].max())\n",
    "        results[\"sim_min\"].append(ss[0][:num].min())\n",
    "        results[\"sim_mean\"].append(ss[0][:num].mean())\n",
    "        results[\"sim_std\"].append(ss[0][:num].std())\n",
    "        results[\"sim_num\"].append(num)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beb14724-e935-457c-b1da-11483682de82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90/2703185024.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df = pd.read_csv(\"input/kaggle-llm-science-exam/test.csv\").drop(\"id\", 1).head(10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【title 検索】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e032e9414af44b39191ddf375674327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(cfg.sim_model, device=\"cuda\")\n",
    "model.max_seq_length = cfg.sim_max_length\n",
    "model = model.half()\n",
    "\n",
    "df = pd.read_csv(\"input/kaggle-llm-science-exam/test.csv\").drop(\"id\", 1).head(10)\n",
    "df[[\"A\", \"B\", \"C\", \"D\", \"E\"]] = df[[\"A\", \"B\", \"C\", \"D\", \"E\"]].fillna(\"\")\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df[\"answer_all\"] = df.apply(lambda x: \" \".join([x[\"A\"], x[\"B\"], x[\"C\"], x[\"D\"], x[\"E\"]]), axis=1)\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt\"] + \" \" + df[\"answer_all\"]\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt_answer_stem\"].str.replace('\"', \"\")\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt_answer_stem\"].str.replace(\"“\", \"\")\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt_answer_stem\"].str.replace(\"”\", \"\")\n",
    "\n",
    "\n",
    "# title 検索\n",
    "print(\"【title 検索】\")\n",
    "search_score, search_index = relevant_title_retrieval(\n",
    "    df,\n",
    "    cfg.index_path,\n",
    "    model,\n",
    "    top_k=cfg.doc_top_k,\n",
    "    batch_size=cfg.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d7edb2-9571-4b5e-a70a-d65ec61e3da8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10069827, 0.12945409, 0.12975068],\n",
       "       [0.13511868, 0.15179165, 0.18356284],\n",
       "       [0.129907  , 0.13123852, 0.14057511],\n",
       "       [0.15040585, 0.16520883, 0.17001614],\n",
       "       [0.14506043, 0.14928862, 0.16130355],\n",
       "       [0.11162052, 0.12615484, 0.14368361],\n",
       "       [0.18993285, 0.19940323, 0.2052996 ],\n",
       "       [0.18377444, 0.18975887, 0.19107112],\n",
       "       [0.17239162, 0.18588975, 0.19105905],\n",
       "       [0.1497725 , 0.1599384 , 0.16583973]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb642a5-0893-4175-987a-67e57980a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【wikipedia file data 取得】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec9dddc7bb54aa8b4b7b05fb84651f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  prompt_id       file\n",
      "0  52303418          1  d.parquet\n",
      "1  21591425          0  m.parquet\n",
      "2  25675405          1  s.parquet\n",
      "【wikipedia text data 取得】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6ef544e7ed4fc58646ef854377f3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                     title  \\\n",
      "0  52303418                           Dynamic scaling   \n",
      "1  21591425               Modified Newtonian dynamics   \n",
      "2  25675405  Self-Similarity of Network Data Analysis   \n",
      "\n",
      "                                                text  \n",
      "0  Dynamic scaling (sometimes known as Family-Vic...  \n",
      "1  Modified Newtonian dynamics (MOND) is a hypoth...  \n",
      "2  In computer networks, self-similarity is a fea...  \n",
      "【sentencize】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b0c22be7834061a6a5e0a194e177d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Get embeddings of the wiki text data】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a60af3c2226491ab2e57fcce1ffcd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Combine all answers】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4813a7960748bab387d124f0fcf3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wikipedia file data 取得 (\"id\", \"prompt_id\", \"file\")\n",
    "print(\"【wikipedia file data 取得】\")\n",
    "wikipedia_file_data = get_wikipedia_file_data(\n",
    "    search_score,\n",
    "    search_index,\n",
    "    cfg.wiki_index_path,\n",
    ")\n",
    "print(wikipedia_file_data.head())\n",
    "\n",
    "# wikipedia text data 取得 (\"id\", \"text\")\n",
    "print(\"【wikipedia text data 取得】\")\n",
    "wiki_text_data = get_full_text_data(\n",
    "    wikipedia_file_data,\n",
    "    cfg.wiki_dir,\n",
    ")\n",
    "print(wiki_text_data.head())\n",
    "\n",
    "## Parse documents into sentences\n",
    "print(\"【sentencize】\")\n",
    "processed_wiki_text_data = sentencize(\n",
    "    wiki_text_data.title.values,\n",
    "    wiki_text_data.text.values,\n",
    "    wiki_text_data.id.values,\n",
    "    cfg.window_size,\n",
    "    cfg.sliding_size,\n",
    ")\n",
    "\n",
    "\n",
    "## Get embeddings of the wiki text data\n",
    "print(\"【Get embeddings of the wiki text data】\")\n",
    "wiki_data_embeddings = model.encode(\n",
    "    processed_wiki_text_data.text,\n",
    "    batch_size=cfg.batch_size,\n",
    "    device=\"cuda\",\n",
    "    show_progress_bar=True,\n",
    "    # convert_to_tensor=True,\n",
    "    normalize_embeddings=True,\n",
    ")  # .half()\n",
    "# wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n",
    "wiki_data_embeddings = wiki_data_embeddings.astype(np.float32)\n",
    "# print data size(GB) of wiki_data_embeddings\n",
    "\n",
    "_ = gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "## Combine all answers\n",
    "print(\"【Combine all answers】\")\n",
    "question_embeddings = model.encode(\n",
    "    df.prompt_answer_stem.values,\n",
    "    batch_size=cfg.batch_size,\n",
    "    device=\"cuda\",\n",
    "    show_progress_bar=True,\n",
    "    # convert_to_tensor=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "question_embeddings = question_embeddings.astype(np.float32)\n",
    "df.drop([\"answer_all\", \"prompt_answer_stem\"], axis=1, inplace=True)\n",
    "# question_embeddings = question_embeddings.detach().cpu().numpy()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692bd7e5-82aa-4a38-ae21-b39f13259617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.1931152e-02,  1.8280029e-02,  1.2161255e-02, -5.9852600e-03,\n",
       "        -1.3183594e-02,  3.3843994e-02,  1.6510010e-02, -1.4901161e-05,\n",
       "         7.9650879e-03, -2.2796631e-02],\n",
       "       [-6.0119629e-02, -1.1695862e-02, -6.3514709e-03, -1.3298035e-02,\n",
       "        -1.5274048e-02,  3.0944824e-02,  9.7198486e-03,  4.4174194e-03,\n",
       "         8.0261230e-03, -2.3040771e-02]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data_embeddings[:2, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdcbaa87-2cb7-4862-8896-27bd57ffde68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05453491, -0.03013611,  0.00688171, -0.02697754,  0.01304626,\n",
       "        -0.0038166 ,  0.00697327,  0.02407837, -0.00566483,  0.02145386],\n",
       "       [-0.04086304, -0.01330566, -0.02146912, -0.01564026,  0.01847839,\n",
       "        -0.00036621,  0.02664185, -0.01551819,  0.03930664, -0.01786804]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeddings[:2, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9daa2e1-652f-41aa-b74d-64f5bdc9f62a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f5d0f0412e4509804b10fa84249698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Extract contexts from matching pairs\n",
    "results = extract_contexts_from_matching_pairs(\n",
    "    df,\n",
    "    processed_wiki_text_data,\n",
    "    wikipedia_file_data,\n",
    "    wiki_data_embeddings,\n",
    "    question_embeddings,\n",
    "    num_sentences_include=cfg.num_sentences_include,\n",
    ")\n",
    "df[\"context\"] = results[\"contexts\"]\n",
    "df[\"sim_max\"] = results[\"sim_max\"]\n",
    "df[\"sim_min\"] = results[\"sim_min\"]\n",
    "df[\"sim_mean\"] = results[\"sim_mean\"]\n",
    "df[\"sim_std\"] = results[\"sim_std\"]\n",
    "df[\"sim_num\"] = results[\"sim_num\"]\n",
    "\n",
    "df.to_csv(\"./test_context.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e0535a-dc34-4864-adea-11d1f736aea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238b3b05d1f7418587105cef4dc08571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# infer\n",
    "class Config:\n",
    "    # infer\n",
    "    max_length = 150\n",
    "    max_length_valid = 300\n",
    "    model_path = \"dataset/llm-science-models/300_000\"\n",
    "    sep_token = \"['SEP']\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForMultipleChoice, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace('\"', \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"test_context.csv\")\n",
    "\n",
    "\n",
    "def preprocess_df(df, mode=\"train\"):\n",
    "    max_length = cfg.max_length if mode == \"train\" else cfg.max_length_valid  # 推論時はtokenを長く取る\n",
    "    df[\"prompt_with_context\"] = (\n",
    "        df[\"context\"].apply(lambda x: \" \".join(x.split()[:max_length])) + f\"... {cfg.sep_token} \" + df[\"prompt\"]\n",
    "    )\n",
    "    df[\"prompt_with_context\"] = df[\"prompt_with_context\"].apply(clean_text)\n",
    "\n",
    "    # 空を埋める\n",
    "    options = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "    for option in options:\n",
    "        df[option] = df[option].fillna(\"\")\n",
    "    return df\n",
    "\n",
    "\n",
    "test_df = preprocess_df(test_df, mode=\"valid\")\n",
    "test_df[\"answer\"] = \"A\"\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)\n",
    "option_to_index = {option: idx for idx, option in enumerate(\"ABCDE\")}\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [example[\"prompt_with_context\"]] * 5\n",
    "    second_sentences = [example[option] for option in \"ABCDE\"]\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=False)\n",
    "    tokenized_example[\"label\"] = option_to_index[example[\"answer\"]]\n",
    "\n",
    "    return tokenized_example\n",
    "\n",
    "\n",
    "tokenized_test_ds = test_ds.map(\n",
    "    preprocess,\n",
    "    batched=False,\n",
    "    remove_columns=[\"prompt_with_context\", \"prompt\", \"A\", \"B\", \"C\", \"D\", \"E\", \"answer\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(cfg.model_path)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "args = TrainingArguments(output_dir=\"output/tmp\", per_device_eval_batch_size=1)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(\n",
    "        tokenizer=tokenizer,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "test_predictions = trainer.predict(tokenized_test_ds).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3d71b8-6755-4169-a593-42cfb7066587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4838045 , -1.6397529 , -1.1198299 ,  9.401418  ,  1.4131405 ],\n",
       "       [ 2.8451388 ,  0.62608135, -1.232299  , -0.84630734, -0.59199804]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67081557-e3b3-49bf-8143-81e0a946248b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
