{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77cc53de-7819-4959-8ea4-b993a9bf3970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/working'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "242d34ae-807b-472a-ac57-317aabb51b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ctypes\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections.abc import Iterable\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import blingfire as bf\n",
    "import faiss\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from faiss import read_index, write_index\n",
    "from hydra import compose, initialize, initialize_config_dir, initialize_config_module\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d22c9a-25c6-49fa-b295-2df8368bca46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_paths': ['preprocessed/901_concat/data2.csv', 'preprocessed/901_concat/data1.csv', 'preprocessed/901_concat/data0_0.csv', 'preprocessed/901_concat/data0_10000.csv', 'preprocessed/901_concat/data0_20000.csv', 'preprocessed/901_concat/data0_30000.csv', 'preprocessed/901_concat/data0_40000.csv', 'preprocessed/901_concat/data0_50000.csv', 'preprocessed/901_concat/data0_60000.csv', 'preprocessed/901_concat/data0_70000.csv', 'preprocessed/901_concat/data0_80000.csv', 'preprocessed/901_concat/data0_90000.csv'], 'wiki_dir': 'input/llm-science-wikipedia-data-b', 'wiki_index_path': 'preprocessed/320_doc_index/001/all.parquet', 'index_path': 'preprocessed/320_doc_index/001/ivfpq_index.faiss', 'sim_model': 'BAAI/bge-small-en', 'num_sentences_include': 20, 'max_length': 384, 'batch_size': 32, 'doc_top_k': 3, 'window_size': 5, 'sliding_size': 4, 'debug': False, 'seed': 7}\n",
      "preprocessed_path: preprocessed/331/000\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"../yamls\"):\n",
    "    c = compose(config_name=\"config.yaml\", overrides=[\"preprocess=331/000\"])\n",
    "\n",
    "    OmegaConf.resolve(c)  # debugやseedを解決\n",
    "    cfg = c.preprocess\n",
    "\n",
    "# debugやseedを解決\n",
    "cfg = c.preprocess\n",
    "\n",
    "exp_name = f\"331/000\"\n",
    "preprocessed_path = Path(f\"./preprocessed/{exp_name}\")\n",
    "\n",
    "print(cfg)\n",
    "print(\"preprocessed_path:\", preprocessed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df8e6a6b-cb1c-4d58-b83d-98db951919f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_chunk_by_sliding_window(text_list: list[str], window_size: int, sliding_size: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    text のリストをsliding windowで結合する。window_size個のtextが含まれるまで結合し、sliding_size個ずつずらして結合する。\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text_list), sliding_size):\n",
    "        chunk = \" \".join(text_list[i : i + window_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_sections(text: str) -> list[tuple[str, str]]:\n",
    "    pattern = re.compile(r\"#{2,}\\s?(.*?)\\s?#{2,}\")\n",
    "    sections = []\n",
    "\n",
    "    matches = list(pattern.finditer(text))\n",
    "    start_idx = 0\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        sections.append((\"\", text))\n",
    "        return sections\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        if i == 0:\n",
    "            end_idx = match.start()\n",
    "            sections.append((\"\", text[start_idx:end_idx].strip()))\n",
    "\n",
    "        start_idx = match.end()\n",
    "        end_idx = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "        section = (match.group(1).strip(), text[start_idx:end_idx].strip())\n",
    "        if section[0] not in [\"See also\", \"References\", \"Further reading\", \"External links\"]:\n",
    "            sections.append(section)\n",
    "\n",
    "        start_idx = end_idx\n",
    "\n",
    "    # 空のtextの場合は飛ばす\n",
    "    sections = [section for section in sections if len(section[1].split(\" \")) >= 3]\n",
    "    return sections\n",
    "\n",
    "\n",
    "def sentencize(\n",
    "    titles: Iterable[str],\n",
    "    documents: Iterable[str],\n",
    "    document_ids: Iterable,\n",
    "    window_size: int = 3,\n",
    "    sliding_size: int = 2,\n",
    "    filter_len: int = 5,\n",
    "    filter_len_max: int = 500,\n",
    "    disable_progress_bar: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into sentences. Can be used with `sectionize_documents`\n",
    "    to further split documents into more manageable pieces. Takes in offsets\n",
    "    to ensure that after splitting, the sentences can be matched to the\n",
    "    location in the original documents.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    document_sentences = []\n",
    "    for title, document, document_id in tqdm(\n",
    "        zip(titles, documents, document_ids), total=len(documents), disable=disable_progress_bar\n",
    "    ):\n",
    "        try:\n",
    "            # chunk にまとめる\n",
    "            ## 念のため改行をスペースに変換\n",
    "            document = document.replace(\"\\n\", \" \")\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            section_sentences = []\n",
    "            for o in sentence_offsets:\n",
    "                if filter_len < o[1] - o[0] and o[1] - o[0] < filter_len_max:\n",
    "                    section_sentences.append(document[o[0] : o[1]])\n",
    "            chunks = extract_chunk_by_sliding_window(section_sentences, window_size, sliding_size)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                row = {}\n",
    "                row[\"document_id\"] = document_id\n",
    "                row[\"text\"] = f\"{title} > {chunk}\"\n",
    "                row[\"offset\"] = (0, 0)\n",
    "                document_sentences.append(row)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(document_sentences)\n",
    "\n",
    "\n",
    "def sectionize_documents(\n",
    "    titles: Iterable[str],\n",
    "    documents: Iterable[str],\n",
    "    document_ids: Iterable,\n",
    "    disable_progress_bar: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the\n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for title, document_id, document in tqdm(\n",
    "        zip(titles, document_ids, documents), total=len(documents), disable=disable_progress_bar\n",
    "    ):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row[\"document_id\"] = document_id\n",
    "        row[\"text\"] = text\n",
    "        row[\"offset\"] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values([\"document_id\", \"offset\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def relevant_title_retrieval(\n",
    "    df: pd.DataFrame,\n",
    "    index_path: str,\n",
    "    model: SentenceTransformer,\n",
    "    top_k: int = 3,\n",
    "    batch_size: int = 32,\n",
    ") -> pd.DataFrame:\n",
    "    sentence_index = read_index(index_path)  # index 読み込み\n",
    "    res = faiss.StandardGpuResources()  # use a single GPU\n",
    "    co = faiss.GpuClonerOptions()\n",
    "    co.useFloat16 = True\n",
    "    sentence_index = faiss.index_cpu_to_gpu(res, 0, sentence_index, co)\n",
    "    sentence_index.nprobe = 10\n",
    "    prompt_embeddings = model.encode(\n",
    "        df.prompt_answer_stem.values,\n",
    "        batch_size=batch_size,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        # convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    prompt_embeddings = prompt_embeddings.astype(np.float32)\n",
    "    # prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "    search_score, search_index = sentence_index.search(prompt_embeddings, top_k)\n",
    "    res.noTempMemory()\n",
    "    del res\n",
    "    del sentence_index\n",
    "    del prompt_embeddings\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return search_score, search_index\n",
    "\n",
    "\n",
    "def get_wikipedia_file_data(\n",
    "    search_score: np.ndarray,\n",
    "    search_index: np.ndarray,\n",
    "    wiki_index_path: str,\n",
    ") -> pd.DataFrame:\n",
    "    wiki_index_df = pd.read_parquet(wiki_index_path, columns=[\"id\", \"file\"])\n",
    "    wikipedia_file_data = []\n",
    "    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n",
    "        _df = wiki_index_df.loc[idx].copy()\n",
    "        _df[\"prompt_id\"] = i\n",
    "        wikipedia_file_data.append(_df)\n",
    "    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "    wikipedia_file_data = (\n",
    "        wikipedia_file_data[[\"id\", \"prompt_id\", \"file\"]]\n",
    "        .drop_duplicates()\n",
    "        .sort_values([\"file\", \"id\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    ## Save memory - delete df since it is no longer necessary\n",
    "    del wiki_index_df\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return wikipedia_file_data\n",
    "\n",
    "\n",
    "def get_full_text_data(\n",
    "    wikipedia_file_data: pd.DataFrame,\n",
    "    wiki_dir: str,\n",
    "):\n",
    "    ## Get the full text data\n",
    "    wiki_text_data = []\n",
    "    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n",
    "        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data[\"file\"] == file][\"id\"].tolist()]\n",
    "        _df = pd.read_parquet(f\"{wiki_dir}/{file}\", columns=[\"id\", \"title\", \"text\"])\n",
    "        _df_temp = _df[_df[\"id\"].isin(_id)].copy()\n",
    "        del _df\n",
    "        _ = gc.collect()\n",
    "        libc.malloc_trim(0)\n",
    "        wiki_text_data.append(_df_temp)\n",
    "    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    return wiki_text_data\n",
    "\n",
    "\n",
    "def extract_contexts_from_matching_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    processed_wiki_text_data: pd.DataFrame,\n",
    "    wikipedia_file_data: pd.DataFrame,\n",
    "    wiki_data_embeddings: np.ndarray,\n",
    "    question_embeddings: np.ndarray,\n",
    "    num_sentences_include: int = 5,\n",
    "):\n",
    "    results = {\"contexts\": [], \"sim_min\": [], \"sim_max\": [], \"sim_mean\": [], \"sim_std\": [], \"sim_num\": []}\n",
    "    for r in tqdm(df.itertuples(), total=len(df)):\n",
    "        prompt_id = r.Index\n",
    "        prompt_indices = processed_wiki_text_data[\n",
    "            processed_wiki_text_data[\"document_id\"].isin(\n",
    "                wikipedia_file_data[wikipedia_file_data[\"prompt_id\"] == prompt_id][\"id\"].values\n",
    "            )\n",
    "        ].index.values\n",
    "        assert prompt_indices.shape[0] > 0\n",
    "        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "        prompt_index.add(wiki_data_embeddings[prompt_indices])\n",
    "        ## Get the top matches\n",
    "        ss, ii = prompt_index.search(question_embeddings[np.newaxis, prompt_id], num_sentences_include)\n",
    "        context = \"\"\n",
    "        total_len = 0\n",
    "        num = 0\n",
    "        for _s, _i in zip(ss[0], ii[0]):\n",
    "            if total_len > 1000 or _s >= 1.0:\n",
    "                break\n",
    "            text = processed_wiki_text_data.loc[prompt_indices][\"text\"].iloc[_i]\n",
    "            context += text + \" \"\n",
    "            total_len += len(text.split(\" \"))\n",
    "            num += 1\n",
    "        results[\"contexts\"].append(context)\n",
    "        results[\"sim_max\"].append(ss[0][:num].max())\n",
    "        results[\"sim_min\"].append(ss[0][:num].min())\n",
    "        results[\"sim_mean\"].append(ss[0][:num].mean())\n",
    "        results[\"sim_std\"].append(ss[0][:num].std())\n",
    "        results[\"sim_num\"].append(num)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b89fc56-2d34-43b8-99c1-921c231ce2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# モデル読み込み\n",
    "model = SentenceTransformer(cfg.sim_model, device=\"cuda\")\n",
    "model.max_seq_length = cfg.max_length\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160680db-0d52-4cf3-a8db-e10794975071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【title 検索】\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71c30d42103461db273345b22ea125d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"preprocessed/901_concat/data2.csv\"\n",
    "\n",
    "# データ読み込み\n",
    "df = pd.read_csv(path)\n",
    "df[[\"A\", \"B\", \"C\", \"D\", \"E\"]] = df[[\"A\", \"B\", \"C\", \"D\", \"E\"]].fillna(\"\")\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "if cfg.debug:\n",
    "    df = df.head(15)\n",
    "\n",
    "df[\"answer_all\"] = df.apply(lambda x: \" \".join([x[\"A\"], x[\"B\"], x[\"C\"], x[\"D\"], x[\"E\"]]), axis=1)\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt\"] + \" \" + df[\"answer_all\"]\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt_answer_stem\"].str.replace('\"', \"\")\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt_answer_stem\"].str.replace(\"“\", \"\")\n",
    "df[\"prompt_answer_stem\"] = df[\"prompt_answer_stem\"].str.replace(\"”\", \"\")\n",
    "\n",
    "# title 検索\n",
    "print(\"【title 検索】\")\n",
    "search_score, search_index = relevant_title_retrieval(\n",
    "    df,\n",
    "    cfg.index_path,\n",
    "    model,\n",
    "    top_k=cfg.doc_top_k,\n",
    "    batch_size=cfg.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "846a1719-da8b-4c24-9fea-ab146f10254a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10069704, 0.12945175, 0.1297493 ],\n",
       "       [0.1351223 , 0.15178347, 0.18356371],\n",
       "       [0.12990236, 0.13123703, 0.14057088],\n",
       "       [0.15044713, 0.16522694, 0.17006326],\n",
       "       [0.14510965, 0.1493355 , 0.16133547]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_score[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6cbf52-6342-4dc8-b38e-60dff8b9f43e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
